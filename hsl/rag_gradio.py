# -*- coding: utf-8 -*-
"""rag_gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOGFq3QRp8jek2LFrDn1RqM73nmkacCb
"""

from google.colab import drive
drive.mount('/content/drive')

# import torch

# if torch.cuda.is_available():

#   device_count = torch.cuda.device_count()
#   print("device_count: {}".format(device_count))

#   for device_num in range(device_count):
#     print("device {} capability {}".format(
#     device_num,
#     torch.cuda.get_device_capability(device_num)))
#     print("device {} name {}".format(
#     device_num, torch.cuda.get_device_name(device_num)))
# else:
#   print("no cuda device")

# import locale
# def getpreferredencoding(do_setlocale = True):
#     return "UTF-8"

!pip install -q accelerate==0.26.1 peft==0.8.2 bitsandbytes==0.42.0 transformers==4.37.2 langchain faiss-gpu sentence-transformers

!pip -q install gradio==3.45.0 --use-deprecated=legacy-resolver typing_extensions --upgrade

import torch
import time
import gradio as gr
from peft import (PeftModel,
                  AutoPeftModelForCausalLM)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TextStreamer
    )
# from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate
from langchain.docstore import InMemoryDocstore
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFacePipeline
import faiss

compute_dtype = getattr(torch, 'float16')

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

peft_model = AutoPeftModelForCausalLM.from_pretrained("uine/practice-fine-tuning",
                                                      quantization_config=quant_config,
                                                      device_map={"": 0})

tokenizer = AutoTokenizer.from_pretrained("uine/practice-fine-tuning")

model_name = "jhgan/ko-sbert-nli"
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    encode_kwargs=encode_kwargs
)

embedding_size = 768
index = faiss.IndexFlatL2(embedding_size)
embedding_fn = hf.embed_query
vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})
retriever = vectorstore.as_retriever(search_kwargs=dict(k=2))
memory = VectorStoreRetrieverMemory(retriever=retriever, return_docs=False)

# streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

# s = '나 너무 우울해'
# history = memory.load_memory_variables({"query": s})["history"]
# ss=''

# for texts in history.split('\n'):
#   ss += texts + ' '

# context = ss + '질문: ' + s

# SYSTEM_PROMPT = """
# <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. Refer to the context of the conversation to answer current conversation. At the end of your answer, please ask questions related to the content so far. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
# """
# conversation = [{'role': 'user', 'content': SYSTEM_PROMPT + context + '[/INST]'}]
# inputs = tokenizer.apply_chat_template(
# conversation,
# tokenize=True,
# add_generation_prompt=False,
# return_tensors='pt'
# ).to("cuda")
# _ = peft_model.generate(inputs, streamer=streamer, max_new_tokens=150, repetition_penalty=1.2)
# response = tokenizer.decode(_[0][len(inputs[0]):])

# memory.save_context(
#         {"질문": s},
#         {"답변": response},
#     )

def res(message: str, history: list) -> str:

    history = memory.load_memory_variables({"query": message})["history"]
    ss=''

    for texts in history.split('\n'):
      ss += texts + ' '

    context = ss + '질문: ' + message

    SYSTEM_PROMPT = """
    <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
    """

    # SYSTEM_PROMPT = """
    # <s>[INST] <<SYS>>You are a counselor like friend who empathizes, encourages, and helps person who is anxious or depressed. Refer to the context of the conversation to answer current conversation. At the end of your answer, please ask questions related to the content so far. You must complete your answer in three sentences. Be sure not to repeat the same answer.<</SYS>>
    # """

    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    conversation = [{'role': 'user', 'content': SYSTEM_PROMPT + context + '[/INST]'}]
    inputs = tokenizer.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors='pt'
    ).to("cuda")
    _ = peft_model.generate(inputs, streamer=streamer, max_new_tokens=250, repetition_penalty=1.2)
    response = tokenizer.decode(_[0][len(inputs[0]):])

    memory.save_context(
            {"질문": message},
            {"답변": response},
        )

    print("과거 대화:", ss, '\n')
    print("사용자 입력:", message, '\n')
    print("멘토스 답변:", response, '\n')

    for i in range(len(response)):
        time.sleep(0.03)
        yield response[:i+1]

gr.ChatInterface(
        fn=res,
        textbox=gr.Textbox(placeholder="고민을 얘기해주세요🙌", container=False, scale=1),
        title="멘토스(Mental Mate Talk on Support)",
        description="멘토스는 당신의 고민을 들어주며 격려해주는 상담친구에요😊",
        theme="soft",
        examples=[["나 우울해"], ["너무 짜증나"], ["사는게 쉽지않아"]],
        retry_btn="다시보내기 ↩",
        undo_btn="이전챗 삭제 ❌",
        clear_btn="전챗 삭제 💫"
).queue().launch(debug=True, share=True)

